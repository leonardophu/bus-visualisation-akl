# Store the dataframes all into one
all_data = lapply(website_url, function(x)
{data = fromJSON(url(x))
return(data.frame(as.Date(data$daily$time), data$daily$temperature_2m_max))
})
# For each dataframe, we want to create a column that contains the city associated with the data
for (i in 1:5) {
all_data[[i]]$city = top_cities$city[i]
}
# Put it all into one dataframe
matrix_data = do.call(rbind, all_data)
colnames(matrix_data) = c("Time", "Temperature", "City")
# Then we can use xtabs to get our dataset into matrix form
matplot_matrix <- xtabs(Temperature ~ Time + City, matrix_data)
# Then we can plot our plot using matplot
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature" , xaxt = "n")
legend("bottomleft", legend = colnames(matplot_matrix), col = 1:5, lty = 1, cex = 0.6)
month_start <- format(matrix_data$Time, "%d") == "01"
axis(1, at = matrix_data$Time[month_start], labels = format(matrix_data$Time[month_start], "%b"))
# It was found via trial and error, that the first table contained the information we wanted
cities = as.data.frame(html_table(tabs[1]))
# Get the top 5 cities
top_cities = cities[order(cities$population, decreasing= TRUE),][1:5,]
# Get all the website URLS that we need
website_url = paste0("https://archive-api.open-meteo.com/v1/archive?latitude=",top_cities$lat,"&longitude=",top_cities$lng,"&start_date=2022-01-01&end_date=2022-12-31&daily=temperature_2m_max&timezone=Pacific%2FAuckland")
# Store the dataframes all into one
all_data = lapply(website_url, function(x)
{data = fromJSON(url(x))
return(data.frame(as.Date(data$daily$time), data$daily$temperature_2m_max))
})
# For each dataframe, we want to create a column that contains the city associated with the data
for (i in 1:5) {
all_data[[i]]$city = top_cities$city[i]
}
# Put it all into one dataframe
matrix_data = do.call(rbind, all_data)
colnames(matrix_data) = c("Time", "Temperature", "City")
# Then we can use xtabs to get our dataset into matrix form
matplot_matrix <- xtabs(Temperature ~ Time + City, matrix_data)
# Then we can plot our plot using matplot
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature" , xaxt = "n")
legend("bottomleft", legend = colnames(matplot_matrix), col = 1:5, lty = 1, cex = 0.6)
# It was found via trial and error, that the first table contained the information we wanted
cities = as.data.frame(html_table(tabs[1]))
# Get the top 5 cities
top_cities = cities[order(cities$population, decreasing= TRUE),][1:5,]
# Get all the website URLS that we need
website_url = paste0("https://archive-api.open-meteo.com/v1/archive?latitude=",top_cities$lat,"&longitude=",top_cities$lng,"&start_date=2022-01-01&end_date=2022-12-31&daily=temperature_2m_max&timezone=Pacific%2FAuckland")
# Store the dataframes all into one
all_data = lapply(website_url, function(x)
{data = fromJSON(url(x))
return(data.frame(as.Date(data$daily$time), data$daily$temperature_2m_max))
})
# For each dataframe, we want to create a column that contains the city associated with the data
for (i in 1:5) {
all_data[[i]]$city = top_cities$city[i]
}
# Put it all into one dataframe
matrix_data = do.call(rbind, all_data)
colnames(matrix_data) = c("Time", "Temperature", "City")
# Then we can use xtabs to get our dataset into matrix form
matplot_matrix <- xtabs(Temperature ~ Time + City, matrix_data)
# Then we can plot our plot using matplot
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
legend("bottomleft", legend = colnames(matplot_matrix), col = 1:5, lty = 1, cex = 0.6)
rownames(matplot_matrix)
# It was found via trial and error, that the first table contained the information we wanted
cities = as.data.frame(html_table(tabs[1]))
# Get the top 5 cities
top_cities = cities[order(cities$population, decreasing= TRUE),][1:5,]
# Get all the website URLS that we need
website_url = paste0("https://archive-api.open-meteo.com/v1/archive?latitude=",top_cities$lat,"&longitude=",top_cities$lng,"&start_date=2022-01-01&end_date=2022-12-31&daily=temperature_2m_max&timezone=Pacific%2FAuckland")
# Store the dataframes all into one
all_data = lapply(website_url, function(x)
{data = fromJSON(url(x))
return(data.frame(as.Date(data$daily$time), data$daily$temperature_2m_max))
})
# For each dataframe, we want to create a column that contains the city associated with the data
for (i in 1:5) {
all_data[[i]]$city = top_cities$city[i]
}
# Put it all into one dataframe
matrix_data = do.call(rbind, all_data)
colnames(matrix_data) = c("Time", "Temperature", "City")
# Then we can use xtabs to get our dataset into matrix form
matplot_matrix <- xtabs(Temperature ~ Time + City, matrix_data)
# Then we can plot our plot using matplot
matplot(format(as.Date(rownames(matplot_matrix)), "%b %d"), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
format(as.Date(rownames(matplot_matrix)), "%b %d")
# It was found via trial and error, that the first table contained the information we wanted
cities = as.data.frame(html_table(tabs[1]))
# Get the top 5 cities
top_cities = cities[order(cities$population, decreasing= TRUE),][1:5,]
# Get all the website URLS that we need
website_url = paste0("https://archive-api.open-meteo.com/v1/archive?latitude=",top_cities$lat,"&longitude=",top_cities$lng,"&start_date=2022-01-01&end_date=2022-12-31&daily=temperature_2m_max&timezone=Pacific%2FAuckland")
# Store the dataframes all into one
all_data = lapply(website_url, function(x)
{data = fromJSON(url(x))
return(data.frame(as.Date(data$daily$time), data$daily$temperature_2m_max))
})
# For each dataframe, we want to create a column that contains the city associated with the data
for (i in 1:5) {
all_data[[i]]$city = top_cities$city[i]
}
# Put it all into one dataframe
matrix_data = do.call(rbind, all_data)
colnames(matrix_data) = c("Time", "Temperature", "City")
# Then we can use xtabs to get our dataset into matrix form
matplot_matrix <- xtabs(Temperature ~ Time + City, matrix_data)
# Then we can plot our plot using matplot
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
legend("bottomleft", legend = colnames(matplot_matrix), col = 1:5, lty = 1, cex = 0.6)
# It was found via trial and error, that the first table contained the information we wanted
cities = as.data.frame(html_table(tabs[1]))
# Get the top 5 cities
top_cities = cities[order(cities$population, decreasing= TRUE),][1:5,]
# Get all the website URLS that we need
website_url = paste0("https://archive-api.open-meteo.com/v1/archive?latitude=",top_cities$lat,"&longitude=",top_cities$lng,"&start_date=2022-01-01&end_date=2022-12-31&daily=temperature_2m_max&timezone=Pacific%2FAuckland")
# Store the dataframes all into one
all_data = lapply(website_url, function(x)
{data = fromJSON(url(x))
return(data.frame(as.Date(data$daily$time), data$daily$temperature_2m_max))
})
# For each dataframe, we want to create a column that contains the city associated with the data
for (i in 1:5) {
all_data[[i]]$city = top_cities$city[i]
}
# Put it all into one dataframe
matrix_data = do.call(rbind, all_data)
colnames(matrix_data) = c("Time", "Temperature", "City")
# Then we can use xtabs to get our dataset into matrix form
matplot_matrix <- xtabs(Temperature ~ Time + City, matrix_data)
# Then we can plot our plot using matplot
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
legend("bottomleft", legend = colnames(matplot_matrix), col = 1:5, lty = 1, cex = 0.6)
?plot
matplot_matrix
range(rownames(matplot_matrix))
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
range(rownames(matplot_matrix))
dim(matplot_matrix)
range(as.Date(rownames(matplot_matrix)))
sessionInfo()
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
axis.Date(1, as.Date(rownames(matplot_matrix)))
str(axis.Date(1, as.Date(rownames(matplot_matrix))))
axis.Date
saveRDS(matplot_matrix, file="/tmp/x.rds")
str(axis.Date(1, as.Date(rownames(matplot_matrix))))
unclass(axis.Date(1, as.Date(rownames(matplot_matrix))))
attributes(axis.Date(1, as.Date(rownames(matplot_matrix))))
axis.Date
a=axis.Date(1, as.Date(rownames(matplot_matrix)))
axis.Date(1, as.Date(rownames(matplot_matrix)), a)
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature")
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature", axis=F)
matplot(as.Date(rownames(matplot_matrix)), matplot_matrix, type = "l", lty= 1, xlab = "Day", ylab = "max. temperature", axes=F)
axis.Date(1, as.Date(rownames(matplot_matrix)), a)
a=axis.Date(1, as.Date(rownames(matplot_matrix)))
stops = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stops.txt", header = TRUE, sep = ",", quote = "")
stop_times = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stop_times.txt", header = TRUE, sep = ",", quote = "")
trips = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/trips.txt", header = TRUE, sep = ",", quote = "", fill = TRUE)
routes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/routes.txt", header = TRUE, sep = ",", quote = "")
shapes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/shapes.txt", header = TRUE, sep = ",", quote = "")
stop_times = stop_times %>% mutate(stop_code = substring(stop_id, 1, regexpr("-", stop_id)[1] - 1))
library(tidyverse)
library(leaflet)
library(jsonlite)
library(httr)
stops = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stops.txt", header = TRUE, sep = ",", quote = "")
stop_times = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stop_times.txt", header = TRUE, sep = ",", quote = "")
trips = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/trips.txt", header = TRUE, sep = ",", quote = "", fill = TRUE)
routes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/routes.txt", header = TRUE, sep = ",", quote = "")
shapes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/shapes.txt", header = TRUE, sep = ",", quote = "")
stop_times = stop_times %>% mutate(stop_code = substring(stop_id, 1, regexpr("-", stop_id)[1] - 1))
stops
stop_times
trips
routes
shapes
stop_times
colnames(stops)
colnames(stop_times)
trips
routes
shapes
stop_times
library(tidyverse)
library(leaflet)
library(jsonlite)
library(httr)
key = "567bb1fb7ab64582905c7812648075e1"
#Reading in static files
stops = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stops.txt", header = TRUE, sep = ",", quote = "")
stop_times = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stop_times.txt", header = TRUE, sep = ",", quote = "")
trips = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/trips.txt", header = TRUE, sep = ",", quote = "", fill = TRUE)
routes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/routes.txt", header = TRUE, sep = ",", quote = "")
shapes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/shapes.txt", header = TRUE, sep = ",", quote = "")
stop_times = stop_times %>% mutate(stop_code = substring(stop_id, 1, regexpr("-", stop_id)[1] - 1))
#colnames(stop_times)
#stop_times %>% filter(trip_id == "1184-07209-37680-2-75bd3e26")
#Stop_times column stop_code connects to stops$id
t = shapes %>% filter(shape_id == "957-12603-1bf4bceb")
t
leaflet(t) %>% addTiles() %>% addPolylines(data = t,  lat = t$shape_pt_lat, lng = t$shape_pt_lon)
#Combining stop times with
#Read in data
#Json File
gtfs_trips<-tryCatch(
GET('https://api.at.govt.nz/realtime/legacy/tripupdates',
accept_json(),
add_headers('Ocp-Apim-Subscription-Key' = key)),
error=function(e) NULL)
trip_content = content(gtfs_trips)[[2]][[2]]
#head(str(trip_content), level = 3)
head(trip_content)
#Helper Functions
n = function(x) if(is.null(x)) NA else x
trip_data = as.data.frame(do.call(rbind, lapply(trip_content,
function(x) c(n(x$trip_update$trip$trip_id),
n(x$trip_update$trip$direction_id),
n(x$trip_update$trip$route_id),
n(x$trip_update$stop_time_update$stop_id),
n(x$trip_update$trip$schedule_relationship),
n(x$trip_update$delay),
n(x$trip_update$stop_time_update$stop_sequence),
n(x$trip_update$stop_time_update$arrival$time),
n(x$trip_update$stop_time_update$arrival$delay),
n(x$trip_update$stop_time_update$departure$time),
n(x$trip_update$stop_time_update$departure$delay)
))))
colnames(trip_data) = c("trip_id",
"direction_id",
"route_id",
"stop_id",
"schedule_relationship",
"delay",
"stop_sequence",
"act_arrival_time",
"arrival_delay",
"act_departure_time",
"act_departure_delay")
#Converting our types to be able to join
trip_data$stop_sequence = as.integer(trip_data$stop_sequence)
trip_data = trip_data %>% mutate(stop_code = as.integer(substring(stop_id, 1, regexpr("-", stop_id)[1] - 1)))
#Cancelled buses
alerts <- tryCatch(
GET('https://api.at.govt.nz/realtime/legacy/servicealerts',
accept_json(),
add_headers('Ocp-Apim-Subscription-Key' = key)),
error=function(e) NULL)
#Now alerts are different
alert_contents = content(alerts)
alert_data = as.data.frame(do.call(rbind, lapply(alert_contents[[2]][[2]], function(x) c(x$id, x$alert$effect, x$alert$header_text$translation[[1]]$text, x$alert$informed_entity[[1]]$trip$trip_id))))
colnames(alert_data) = c("id", "effect", "text", "trip_id")
alert_temp1 = alert_data
alert_temp1$trip_id = ifelse(alert_temp1$trip_id == alert_temp1$id, NA, alert_temp1$trip_id)
cancelled_buses = alert_temp1 %>% filter(is.na(trip_id) == FALSE) %>% select(trip_id) %>% mutate(cancelled = TRUE)
#Getting our full dataset by joining
df = trip_data %>%
left_join(stop_times %>% select("trip_id", "stop_sequence", "arrival_time", "departure_time"),
by = c("trip_id" = "trip_id", "stop_sequence" = "stop_sequence")) %>%
left_join(stops %>% select("stop_id", "stop_lat", "stop_lon"),
by = c("stop_id" = "stop_id")) %>%
left_join(routes %>% select("route_id", "route_short_name"),
by = c("route_id" = "route_id")) %>%
left_join(cancelled_buses,
by = c("trip_id" = "trip_id"))
df = df[order(df$trip_id, df$direction_id, df$stop_sequence, df$arrival_time),]
df
library(tidyverse)
library(leaflet)
library(jsonlite)
library(httr)
key = "567bb1fb7ab64582905c7812648075e1"
#Reading in static files
stops = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stops.txt", header = TRUE, sep = ",", quote = "")
stop_times = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/stop_times.txt", header = TRUE, sep = ",", quote = "")
trips = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/trips.txt", header = TRUE, sep = ",", quote = "", fill = TRUE)
routes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/routes.txt", header = TRUE, sep = ",", quote = "")
shapes = read.table("~/Desktop/2023/Thesis Project/Code for Project/gtfs/shapes.txt", header = TRUE, sep = ",", quote = "")
stop_times = stop_times %>% mutate(stop_code = substring(stop_id, 1, regexpr("-", stop_id)[1] - 1))
#colnames(stop_times)
#stop_times %>% filter(trip_id == "1184-07209-37680-2-75bd3e26")
#Stop_times column stop_code connects to stops$id
t = shapes %>% filter(shape_id == "957-12603-1bf4bceb")
t
leaflet(t) %>% addTiles() %>% addPolylines(data = t,  lat = t$shape_pt_lat, lng = t$shape_pt_lon)
#Combining stop times with
#Read in data
#Json File
gtfs_trips<-tryCatch(
GET('https://api.at.govt.nz/realtime/legacy/tripupdates',
accept_json(),
add_headers('Ocp-Apim-Subscription-Key' = key)),
error=function(e) NULL)
trip_content = content(gtfs_trips)[[2]][[2]]
#head(str(trip_content), level = 3)
head(trip_content)
#Helper Functions
n = function(x) if(is.null(x)) NA else x
trip_data = as.data.frame(do.call(rbind, lapply(trip_content,
function(x) c(n(x$trip_update$trip$trip_id),
n(x$trip_update$trip$direction_id),
n(x$trip_update$trip$route_id),
n(x$trip_update$stop_time_update$stop_id),
n(x$trip_update$trip$schedule_relationship),
n(x$trip_update$delay),
n(x$trip_update$stop_time_update$stop_sequence),
n(x$trip_update$stop_time_update$arrival$time),
n(x$trip_update$stop_time_update$arrival$delay),
n(x$trip_update$stop_time_update$departure$time),
n(x$trip_update$stop_time_update$departure$delay)
))))
colnames(trip_data) = c("trip_id",
"direction_id",
"route_id",
"stop_id",
"schedule_relationship",
"delay",
"stop_sequence",
"act_arrival_time",
"arrival_delay",
"act_departure_time",
"act_departure_delay")
#Converting our types to be able to join
trip_data$stop_sequence = as.integer(trip_data$stop_sequence)
trip_data = trip_data %>% mutate(stop_code = as.integer(substring(stop_id, 1, regexpr("-", stop_id)[1] - 1)))
#Cancelled buses
alerts <- tryCatch(
GET('https://api.at.govt.nz/realtime/legacy/servicealerts',
accept_json(),
add_headers('Ocp-Apim-Subscription-Key' = key)),
error=function(e) NULL)
#Now alerts are different
alert_contents = content(alerts)
alert_data = as.data.frame(do.call(rbind, lapply(alert_contents[[2]][[2]], function(x) c(x$id, x$alert$effect, x$alert$header_text$translation[[1]]$text, x$alert$informed_entity[[1]]$trip$trip_id))))
colnames(alert_data) = c("id", "effect", "text", "trip_id")
alert_temp1 = alert_data
alert_temp1$trip_id = ifelse(alert_temp1$trip_id == alert_temp1$id, NA, alert_temp1$trip_id)
cancelled_buses = alert_temp1 %>% filter(is.na(trip_id) == FALSE) %>% select(trip_id) %>% mutate(cancelled = TRUE)
#Getting our full dataset by joining
df = trip_data %>%
left_join(stop_times %>% select("trip_id", "stop_sequence", "arrival_time", "departure_time"),
by = c("trip_id" = "trip_id", "stop_sequence" = "stop_sequence")) %>%
left_join(stops %>% select("stop_id", "stop_lat", "stop_lon"),
by = c("stop_id" = "stop_id")) %>%
left_join(routes %>% select("route_id", "route_short_name"),
by = c("route_id" = "route_id")) %>%
left_join(cancelled_buses,
by = c("trip_id" = "trip_id"))
df = df[order(df$trip_id, df$direction_id, df$stop_sequence, df$arrival_time),]
df
#Since these times are in total seconds from Jan 1970, we need to convert them to hours for a given day
class(df$act_arrival_time) = c('POSIXt','POSIXct')
class(df$act_departure_time) = c('POSIXt','POSIXct')
df$act_arrival_time
df$act_arrival_time_in_seconds <- as.numeric(df$act_arrival_time)
df$act_arrival_time_in_seconds
df
c(10173376 - 9836781, 107314, - 336606, 3946703 - 3219541)
c(10173376 - 9836781, 107314 - 336606, 3946703 - 3219541)
c(10173376 - 9836781, 107314 - 336606, 3946703 - 3219541, 3946703 + 107314 + 10173376 - (3219541 + 336606 + 9836781))
c(10173376 - 9836781, 107314 - 336606, 3946703 - 3219541, total_change = 3946703 + 107314 + 10173376 - (3219541 + 336606 + 9836781))
c(Uber Change = 10173376 - 9836781, 107314 - 336606, 3946703 - 3219541, total_change = 3946703 + 107314 + 10173376 - (3219541 + 336606 + 9836781))
c(Uber = 10173376 - 9836781, Via = 107314 - 336606, Lyft = 3946703 - 3219541, Total = 3946703 + 107314 + 10173376 - (3219541 + 336606 + 9836781))
bus75 = subset(full_bus_data, route_short_name == "24B")
setwd("~/Desktop/PROJECTSPACE/TESTINGdatabase/extraction")
# Bus extraction
library(tidyverse)
library(leaflet)
library(jsonlite)
library(httr)
key = "567bb1fb7ab64582905c7812648075e1"
#Reading in static files
stops = read.table("../businfo/stops.txt", header = TRUE, sep = ",", quote = "")
stop_times = read.table("../businfo/stop_times.txt", header = TRUE, sep = ",", quote = "")
trips = read.table("../businfo/trips.txt", header = TRUE, sep = ",", quote = "", fill = TRUE)
routes = read.table("../businfo/routes.txt", header = TRUE, sep = ",", quote = "")
shapes = read.table("../businfo/shapes.txt", header = TRUE, sep = ",", quote = "")
stop_times = stop_times %>% mutate(stop_code = substring(stop_id, 1, regexpr("-", stop_id)[1] - 1))
# Original time
seconds = function(x) {
return(as.numeric(difftime(x, ymd_h("1970-01-01"), units = "secs")))
}
extract_data = function(trip_updates, alerts){
trip_content = trip_updates[[2]][[2]]   # All the required information is in a nested list
# Null check is a function that checks if a feature for an entity (individual buses) exist, if they don't we will replace that column as NA
null_check = function(x) if(is.null(x)) NA else x
# Filtering for invalid trip ID entries
trips = trips %>% filter(trip_id != "trip_id")
# Extracting all relevant columns from the trip_updates GTFS file.
trip_data = as.data.frame(do.call(rbind, lapply(trip_content,
function(x) c(null_check(x$trip_update$trip$trip_id[[1]][1]),
null_check(x$trip_update$trip$direction_id),
null_check(x$trip_update$trip$route_id),
null_check(x$trip_update$stop_time_update$stop_id),
null_check(x$trip_update$trip$schedule_relationship),
null_check(x$trip_update$delay),
null_check(x$trip_update$stop_time_update$stop_sequence),
null_check(x$trip_update$stop_time_update$arrival$time),
null_check(x$trip_update$stop_time_update$arrival$delay),
null_check(x$trip_update$stop_time_update$departure$time),
null_check(x$trip_update$stop_time_update$departure$delay)
))))
trip_data = as.data.frame(lapply(trip_data, unlist))    # Converting to data frame
# Setting column names
colnames(trip_data) = c("trip_id",
"direction_id",
"route_id",
"stop_id",
"schedule_relationship",
"delay",
"stop_sequence",
"act_arrival_time",
"arrival_delay",
"act_departure_time",
"act_departure_delay")
trip_data$stop_sequence = as.integer(trip_data$stop_sequence)   # Fixing stop sequence type
# Sorting out cancellations from the Alert dataset
alert_contents = alerts[[2]][[2]]
#Extract the relevant information we want from the alert data set, Here we want the id and the effect (NO SERVICE, "MODIFIED SERVICE", etc)
alert_data = as.data.frame(do.call(rbind, lapply(alert_contents, function(x) c(null_check(x$id),
null_check(x$alert$effect),
null_check(x$alert$header_text$translation[[1]]$text),
null_check(x$alert$informed_entity[[1]]$trip$trip_id)))))
alert_data = as.data.frame(lapply(alert_data, unlist))  # All the required information is in a nested list
colnames(alert_data) = c("id", "effect", "text", "trip_id") # Setting alert dataset column names
# Getting all cancelled busses, i.e. ones with no service
cancelled_buses <- alert_data %>%
filter(effect == "NO_SERVICE")
# Filtering for cancellation
cancelled_buses <- cancelled_buses %>%
# When a bus' text entry is "Cancellation", these indicates the buses were cancelled
filter(grepl("Cancellation", text) == TRUE) %>%
select(trip_id) %>%
mutate(cancelled = TRUE)
# Joining all relevant bus info datasets together
bus_arrivals_full = trip_data %>%
left_join(stop_times %>%
select("trip_id", "stop_sequence", "arrival_time", "departure_time"),
by = c("trip_id" = "trip_id", "stop_sequence" = "stop_sequence")) %>%
left_join(stops %>% select("stop_id", "stop_lat", "stop_lon"),
by = c("stop_id" = "stop_id")) %>%
left_join(routes %>% select("route_id", "route_short_name"),
by = c("route_id" = "route_id")) %>%
left_join(cancelled_buses,
by = c("trip_id" = "trip_id")) %>%
left_join(trips %>% select("trip_id", "shape_id"),
by = c("trip_id" = "trip_id"))
return(bus_arrivals_full)
}
date_= c("2023-05-05")
full_bus_data <- data.frame()
dates_dir <- paste("..", date_, sep = "/")
# Getting unique times for each specific date
times <- list.files(dates_dir, recursive = FALSE)
# Loop to get dataset
for(time in times) {
# For a given time for a given day, go into that file directory
dates_time_dir <- paste(dates_dir, time, sep = "/")
trip_updates <- read_json(paste(dates_time_dir, "tripupdates.json", sep = "/"))
alerts <- read_json(paste(dates_time_dir, "alerts.json", sep = "/"))
# Get the required dataset for a given day
date_time_data <- extract_data(trip_updates, alerts)
# Store the date the data was collected
date_time_data$date <- date_
# Combine date_time_data with the existing combined_data
full_bus_data <- rbind(full_bus_data, date_time_data)
}
# Creating day of week column
full_bus_data <- full_bus_data %>%
mutate(day_of_week = wday(as.Date(date, format = "%Y-%m-%d"), label = TRUE))
# Removing duplicates
full_bus_data = distinct(full_bus_data)
full_bus_data = subset(full_bus_data, !route_short_name %in% c("WEST", "EAST","NORTH", "SOUTH", "GULF"))
# Converting all non-cancelled busses cancellation statuses to false
full_bus_data$cancelled = ifelse(is.na(full_bus_data$cancelled) == TRUE, FALSE, TRUE)
# Some rows don't have both arrival and departure time. If we don't have both, we assume they are the same
full_bus_data$act_arrival_time = ifelse(is.na(full_bus_data$act_arrival_time) == TRUE & is.na(full_bus_data$act_departure_time) == FALSE,
full_bus_data$act_departure_time,
full_bus_data$act_arrival_time)
full_bus_data$act_departure_time = ifelse(is.na(full_bus_data$act_arrival_time) == FALSE & is.na(full_bus_data$act_departure_time) == TRUE,
full_bus_data$act_arrival_time,
full_bus_data$act_departure_time)
full_bus_data$arrival_time = paste(date_,full_bus_data$arrival_time)
full_bus_data$arrival_time = ymd_hms(full_bus_data$arrival_time, tz = "UTC")
full_bus_data$arrival_time = as.numeric(full_bus_data$arrival_time)
full_bus_data$arrival_time = full_bus_data$arrival_time + 12*3600
full_bus_data$arrival_fixed = full_bus_data$act_arrival_time + 43200
full_bus_data$act_arrival_time_date = as.POSIXct(full_bus_data$arrival_fixed, origin = "1970-01-01", tz = "UTC")
View(full_bus_data %>% group_by(trip_id) %>% arrange(trip_id, stop_sequence))
# Playing around with data
full_bus_data$status = ifelse(full_bus_data$cancelled == TRUE, 0,
ifelse(full_bus_data$delay < 300, 1,
ifelse(full_bus_data$delay < 600, 2, 3)))
bus75 = subset(full_bus_data, route_short_name == "24B")
data75 = bus75 %>% group_by(trip_id) %>% arrange(trip_id, act_arrival_time)
unique(data75$trip_id)
bus75 = subset(full_bus_data, route_short_name == "24B")
data75 = bus75 %>% group_by(trip_id) %>% arrange(trip_id, act_arrival_time)
trial = subset(data75, trip_id == "24-02404-64020-2-64484aca")
trial$timestamps = trial$arrival_fixed
send = trial %>% select("trip_id", "shape_id", "timestamps", "status", "stop_lat", "stop_lon", "route_id", "route_short_name")
send
trial
bus75 = subset(full_bus_data, route_short_name == "24B")
data75 = bus75 %>% group_by(trip_id) %>% arrange(trip_id, act_arrival_time)
trial = subset(data75, trip_id == "24-02404-64020-2-64484aca")
trial$timestamps = trial$arrival_fixed
send = trial %>% select("trip_id", "shape_id", "timestamps", "status", "stop_lat", "stop_lon", "route_id", "route_short_name")
write.csv(send, file = "bus24B.csv", row.names = FALSE)
